import mysql.connector
import os
import requests
from bs4 import BeautifulSoup
import pandas as pd
from urllib.parse import urlparse, unquote

# Function to extract product name from URL


def extract_product_name(url):
    parsed_url = urlparse(url)
    # Split the path of the URL and decode each part
    decoded_parts = [unquote(part)
                     for part in parsed_url.path.split('/') if part]
    # The product name is usually the last part of the path
    product_name = decoded_parts[-1] if decoded_parts else ''
    return product_name

# Function to create tables if they don't exist


def create_tables(cursor):
    # Create table for products
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS products (
            id INT AUTO_INCREMENT PRIMARY KEY,
            product_name VARCHAR(255) UNIQUE
        )
    """)

    # Create table for nutrition facts
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS nutrition_facts (
            id INT AUTO_INCREMENT PRIMARY KEY,
            product_id INT,
            nutrient VARCHAR(255),
            as_sold_100g_ml VARCHAR(255),
            as_sold_per_serving VARCHAR(255),
            comparison VARCHAR(255),
            FOREIGN KEY (product_id) REFERENCES products(id)
        )
    """)

# Function to scrape nutrition facts table from a given URL


def scrape_nutrition_facts_table(url):
    # Send a GET request to the URL
    response = requests.get(url)

    # Parse the HTML content
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find the table with aria-label="Nutrition facts"
    table = soup.find('table', {'aria-label': 'Nutrition facts'})

    # Check if the table is found
    if table:
        # Extract table headers
        headers = [th.get_text(strip=True) for th in table.find(
            'thead').find('tr').find_all('th')]

        # Extract table rows
        rows = []
        for row in table.find('tbody').find_all('tr'):
            # Extract data from each cell in the row
            cells = [cell.get_text(strip=True) for cell in row.find_all('td')]
            rows.append(cells)

        # Create a DataFrame
        df = pd.DataFrame(rows, columns=headers)

        # Replace the problematic text in the DataFrame
        df = df.replace("Fruitsâ€š vegetablesâ€š nuts and rapeseedâ€š walnut and olive oils (estimate from ingredients list analysis)",
                        "Fruits, vegetables, nuts and rapeseed, walnut and olive oils (estimate from ingredients list analysis)")

        return df
    else:
        print(f"Table not found on the page: {url}")
        return None

# Function to insert data into tables


def insert_data(cursor, product_name, nutrition_df):
    try:
        # Insert product into 'products' table (IGNORE if duplicate)
        cursor.execute(
            "INSERT IGNORE INTO products (product_name) VALUES (%s)", (product_name,))

        # Get the product_id of the inserted or existing product
        cursor.execute(
            "SELECT id FROM products WHERE product_name = %s", (product_name,))
        product_id = cursor.fetchone()[0]

        # Insert nutrition facts into 'nutrition_facts' table
        for index, row in nutrition_df.iterrows():
            cursor.execute("""
                INSERT INTO nutrition_facts (product_id, nutrient, as_sold_100g_ml, as_sold_per_serving, comparison)
                VALUES (%s, %s, %s, %s, %s)
            """, (product_id, row.get("Nutrition facts", None), row.get("As soldfor 100 g / 100 ml", None), row.get("As soldper serving (1 L)", None), row.get("Compared to", None)))

        # Commit the transaction
        conn.commit()
        print("Data inserted successfully.")
    except mysql.connector.Error as err:
        print("Error:", err)
        # Rollback the transaction if an error occurs
        conn.rollback()


# Connect to MySQL
conn = mysql.connector.connect(
    host="localhost",
    user="root",
    password="",
    database="nutrition_facts"
)

# Create a cursor
cursor = conn.cursor()

# Create tables if they don't exist
create_tables(cursor)

# List of URLs to scrape
urls = [
    "https://world.openfoodfacts.org/product/3274080005003/eau-de-source-cristaline",
    'https://world.openfoodfacts.org//product/5449000214799/coca-cola-zero',
    'https://world.openfoodfacts.org//product/3451080155161',
    'https://world.openfoodfacts.org//product/5059697734953/roast-chicken-gravy-tesco',
    # Add more URLs here
]

# Iterate over each URL
for url in urls:
    print(f"Scraping data from: {url}")
    # Scrape the table
    nutrition_df = scrape_nutrition_facts_table(url)

    # Check if the DataFrame is not None
    if nutrition_df is not None:
        # Extract product name
        product_name = extract_product_name(url)

        # Insert data into tables
        insert_data(cursor, product_name, nutrition_df)
        print("Data inserted into tables")

# Commit changes and close connection
conn.commit()
conn.close()

print("All data inserted into tables")
